{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Model Evaluation\n",
    "\n",
    "## Examples:\n",
    "\n",
    "Imagine you're bringing coffee to meeting, and you need to predict whether each person at the meeting will want a coffee or not. Which metric should you choose? It depends\n",
    "\n",
    "Outcomes:\n",
    "\n",
    "- FP: Buy a coffee for someone who won't drink it\n",
    "- FN: Don't buy a coffee for someone who wanted one\n",
    "- TP: Buy a coffee for someone who will drink it\n",
    "- TN: Don't buy a coffee for someone who wouldn't drink it anyway\n",
    "\n",
    "Scenarios\n",
    "\n",
    "- lola: really good coffee, but super expensive\n",
    "    - cost of a FP is higher than FN\n",
    "    - precision is better here because buying a cup of coffee for someone who won't drink it is expensive\n",
    "    - We want to be sure about our positive predictions\n",
    "- taco cabana: bad coffee, but cheap\n",
    "    - cost of a FN is higher than FP\n",
    "    - recall because the coffee is cheap, its not bad to buy a cheap coffee for someone who won't drink it; worse to not get someone coffee who wanted it\n",
    "- meeting with super important client\n",
    "    - cost of FN is higher, because they might be offended if we dont' get them coffee\n",
    "    - cost of FN == not signing a contract\n",
    "    - recall\n",
    "\n",
    "What if we just don't buy coffee or buy coffee for everyone? Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini Exercise\n",
    "\n",
    "Scenario: Build a classifier to predict whether a given face should unlock the iPhone.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "- TP: Correct face unlocks phone\n",
    "- TN: correct face does not unlock phone\n",
    "- FP: Incorrct face Unlocks phone,\n",
    "- FN: Incorrect face does not unlock phone\n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes?\n",
    "- FP most costly\n",
    "- Which metric should we use?\n",
    "- measure for precision\n",
    "Scenario: Predict whether an email is spam or not. Emails marked as spam skip the inbox and go to the spam folder.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "- TP: Spam into spam folder\n",
    "- TN: Spam into inbox\n",
    "- FP: Not spam into Spam\n",
    "- FN: Not spam into inbox\n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes?\n",
    "- TN AND FP are costly\n",
    "- Which metric should we use?\n",
    "-  measure for accuracy\n",
    "\n",
    "Scenario: Predict whether an email is a phishing attempt. When we predict positive, show an additional banner warning the user that this might be a phishing email.\n",
    "\n",
    "- What is the positive and negative case?\n",
    "- TP: Phishing warning when phishing\n",
    "- TN: no Phishing warning when phishing\n",
    "- FP: phishing warning when not phishing\n",
    "- FN: no phishing warning when not phishing \n",
    "- What are the possible outcomes?\n",
    "- What are the costs of the outcomes?\n",
    "- TN and \n",
    "- Which metric should we use?\n",
    "- accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'actual': ['coffee', 'no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'coffee'],\n",
    "    'prediction': ['no coffee', 'no coffee', 'coffee', 'coffee', 'coffee', 'coffee', 'no coffee', 'no coffee'],\n",
    "})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df.prediction, df.actual)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TP: predicted coffee + actual is coffee\n",
    "- FP: predicted coffee, but they didn't like coffee\n",
    "- FN: predicted no coffee, but really they liked coffee\n",
    "- TN: predicted no coffee, actual no coffee\n",
    "\n",
    "Note:\n",
    "\n",
    "- our choice of positive and negative is arbitrary\n",
    "- the labels / layout of the confusion matrix varies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "- **accuracy**: (TP + TN) / (TP + TN + FP + FN)\n",
    "    - (3 + 2) / (3 + 1 + 2 +2) = 62.5%\n",
    "- **precision**: TP / (TP + FP)\n",
    "    - 3 / (3 + 1) = 75%\n",
    "    - FP is more costly than FN\n",
    "- **recall**: TP / (TP + FN)\n",
    "    - 3 / (3 + 2) = 60%\n",
    "    - FN is more costly than FP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background: rgba(0, 150, 0, .25); padding: 1em 3em;\">\n",
    "    <p style=\"font-weight: bold\">Sidebar: Baseline Models</p>\n",
    "    <p>A <em>baseline model</em> is a model that we can compare to in order to see if the models we are creating are worthwhile.</p>\n",
    "    <p>Depending on the context, this can mean either an existing rule-based model created with domain knowlege, or a model that makes predictions without any knowledge of the independent variables.</p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.actual.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['baseline'] = 'coffee'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Metric Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model accuracy\n",
    "(df.actual == df.prediction).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline accuracy\n",
    "(df.actual == df.baseline).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision -- how good are our positive predictions\n",
    "# precision -- model performance | pred +\n",
    "subset = df[df.prediction == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall -- how often do we get the actual positive cases\n",
    "# recall -- model performance | actual +\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.prediction == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will the precision and recall of our baseline model that always predicts + be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision\n",
    "subset = df[df.baseline == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n",
    "subset = df[df.actual == 'coffee']\n",
    "print(subset)\n",
    "(subset.baseline == subset.actual).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What does \"positive\" mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = 'coffee'\n",
    "\n",
    "# accuracy -- overall hit rate\n",
    "model_accuracy = (df.prediction == df.actual).mean()\n",
    "baseline_accuracy = (df.baseline == df.actual).mean()\n",
    "\n",
    "# precision -- how good are our positive predictions?\n",
    "# precision -- model performance | predicted positive\n",
    "subset = df[df.prediction == positive]\n",
    "model_precision = (subset.prediction == subset.actual).mean()\n",
    "subset = df[df.baseline == positive]\n",
    "baseline_precision = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "# recall -- how good are we at detecting actual positives?\n",
    "# recall -- model performance | actual positive\n",
    "subset = df[df.actual == positive]\n",
    "model_recall = (subset.prediction == subset.actual).mean()\n",
    "baseline_recall = (subset.baseline == subset.actual).mean()\n",
    "\n",
    "\n",
    "print(f'''\n",
    "positive: {positive}\n",
    "\n",
    "         | accuracy | recall | precision\n",
    "         | -------- | ------ | ---------         \n",
    "   model | {model_accuracy:8.1%} | {model_recall:6.1%} | {model_precision:9.1%}\n",
    "baseline | {baseline_accuracy:8.1%} | {baseline_recall:6.1%} | {baseline_precision:9.1%}\n",
    "''')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
